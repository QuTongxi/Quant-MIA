# Configuration for training parameters
Full:
  n_shadows: null         # Number of shadow models (used for privacy-related experiments)
  shadow_id: null         # ID of the current shadow model
  pkeep: null             # Proportion of the dataset to keep
  savedir: null           # Directory to save the model
  seed: null              # Random seed for reproducibility
  dataset: null           # Dataset name (e.g., cifar10, cifar100, tiny-imagenet)
  datapath: null          # Path to the dataset
  lr: 0.1                 # Learning rate
  epochs: 1             # Number of training epochs
  model: "resnet18"       # Model architecture
  n_queries: 2            # Number of queries

# Configuration for OBC parameters
OBC:
  load: null              # Path to load the model
  datapath: null          # Path to the dataset
  seed: null              # Random seed
  save: null              # Directory to save the model
  wbits: null             # Bitwidth for weight quantization
  asym: null              # Whether to use asymmetric quantization
  sparse-dir: null        # Directory to save the sparse model
  keep: null              # Path to the dataset keep file
  dataset: null           # Dataset name
  last_layer_8bit: null   # Whether to quantize the last layer to 8 bits
  wminmax: false          # Whether to use min-max quantization for weights
  actsym: false           # Whether to use symmetric quantization for activations
  aminmax: false          # Whether to use min-max quantization for activations
  model: "rn18"           # Model architecture
  compress: "quant"       # Compression method (e.g., quantization, pruning)
  nsamples: 2048          # Number of samples in the calibration dataset
  batchsize: -1           # Batch size
  workers: 8              # Number of data loading workers
  nrounds: -1             # Number of training rounds
  noaug: false            # Whether to disable data augmentation
  abits: 32               # Bitwidth for activation quantization
  wperweight: false       # Whether to quantize weights individually
  rel-damp: 0             # Relative damping coefficient
  prunen: 2               # Pruning parameter N
  prunem: 4               # Pruning parameter M
  blocked_size: 4         # Block size for block pruning
  min-sparsity: 0         # Minimum sparsity level
  max-sparsity: 0         # Maximum sparsity level
  delta-sparse: 0         # Sparsity increment step
  nqueries: 2             # Number of queries
  bnt-batches: 100        # Number of batches for BatchNorm adjustment

# Configuration for BRECQ parameters
BRECQ:
  seed: null              # Random seed
  datapath: null          # Path to the dataset
  save: null              # Directory to save the model
  load: null              # Path to load the model
  wbits: null             # Bitwidth for weight quantization
  asym: null              # Whether to use asymmetric quantization
  keep: null              # Path to the dataset keep file
  dataset: null           # Dataset name
  last_layer_8bit: null   # Whether to quantize the last layer to 8 bits
  arch: "resnet18"        # Model architecture
  batch_size: 64          # Batch size for data loading
  workers: 4              # Number of data loading workers
  channel_wise: true      # Whether to apply channel-wise quantization
  n_bits_a: 8             # Bitwidth for activation quantization
  act_quant: false        # Whether to quantize activations
  set_8bit_head_stem: false # Whether to quantize the head and stem layers to 8 bits
  test_before_calibration: false # Whether to test the model before calibration
  num_samples: 1024       # Number of samples in the calibration dataset
  iters_w: 20000          # Number of iterations for weight quantization
  weight: 0.01            # Weight of the quantization loss
  b_start: 20             # Initial temperature for calibration
  b_end: 2                # Final temperature for calibration
  warmup: 0.2             # Warmup period for calibration
  step: 20                # Step size for calibration
  iters_a: 5000           # Number of iterations for activation quantization
  lr: 0.0004              # Learning rate
  p: 2.4                  # L_p norm parameter
  nqueries: 2             # Number of queries

# Configuration for AdaRound parameters
AdaRound:
  seed: null              # Random seed
  load: null              # Path to load the model
  datapath: null          # Path to the dataset
  wbits: null             # Bitwidth for weight quantization
  save: null              # Directory to save the model
  asym: null              # Whether to use asymmetric quantization
  keep: null              # Path to the dataset keep file
  dataset: null           # Dataset name
  last_layer_8bit: null   # Whether to quantize the last layer to 8 bits

